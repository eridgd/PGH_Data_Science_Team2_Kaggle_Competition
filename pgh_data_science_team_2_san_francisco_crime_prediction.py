# -*- coding: utf-8 -*-
"""PGH Data Science Team #2 - San Francisco Crime Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wouknnlb28L9LfzryQzSa2reZNkGi01z

# Download data and install libs
"""

!wget -nc -O train.csv https://www.dropbox.com/s/bx1n4arnm7eoj5u/train.csv?dl=1
!wget -nc -O test.csv https://www.dropbox.com/s/ypb2ve9aiq73uch/test.csv?dl=1
!wget -nc -q https://raw.githubusercontent.com/codeforamerica/click_that_hood/master/public/data/san-francisco.geojson

!pip install shapely geopandas

import numpy as np
import pandas  as pd
import datetime
import time
from itertools import product
from tqdm.autonotebook import tqdm
from sklearn.metrics import f1_score

import torch
import torch.nn as nn
import torch.nn.functional as F

from shapely.geometry import Point
import geopandas as gpd
from geopandas import GeoDataFrame

"""# Preprocess data"""

df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')

df_train.head()

# analyse the first 5 rows of test data
df_test.head()

df_train['split'] = 'train'
df_test['split'] = 'test'

"""## Filter samples with invalid lat/long from train set"""

df_train = df_train[abs(df_train['Y']) < 90]

"""## Replace invalid lat/long in test set with district median"""

district_medians = df_test.groupby('PdDistrict').agg('median').to_dict()
district_medians

invalid_mask = abs(df_test['Y'] >= 90)
df_test.loc[invalid_mask, 'X'] = df_test[invalid_mask]['PdDistrict'].apply(district_medians['X'].get)
df_test.loc[invalid_mask, 'Y'] = df_test[invalid_mask]['PdDistrict'].apply(district_medians['Y'].get)

"""## Merge train/test DFs"""

df = pd.concat([df_train, df_test])

"""## Parse Dates column as datetime"""

df['Dates'] = pd.to_datetime(df['Dates'])

print(f"train min: {df[df.split=='train'].Dates.min()} max: {df[df.split=='train'].Dates.max()}")
print(f"test min: {df[df.split=='test'].Dates.min()} max: {df[df.split=='test'].Dates.max()}")

"""## Sort DF by date"""

df = df.sort_values('Dates')

df.iloc[:3]

df.iloc[-3:]

"""## Assign node ID by sort order"""

df['node'] = range(len(df))

"""## Process temporal & categorical features"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
pd.DataFrame({'x': np.sin((2*np.pi) / 24 * np.arange(0,24)), 
              'y': np.cos((2*np.pi) / 24 * np.arange(0,24))}) \
              .plot(x='x', y='y', kind='scatter').set_aspect('equal')

def datetime2circle(dt, period):
    # See https://datascience.stackexchange.com/questions/5990/what-is-a-good-way-to-transform-cyclic-ordinal-attributes
    dt_sin = np.sin(2*np.pi * dt / period)
    dt_cos = np.cos(2*np.pi * dt / period)
    return dt_sin, dt_cos

hour_sin, hour_cos = datetime2circle(df['Dates'].dt.hour, 24)
day_of_week_sin, day_of_week_cos = datetime2circle(df['Dates'].dt.dayofweek, 7)
week_of_year_sin, week_of_year_cos = datetime2circle(df['Dates'].dt.weekofyear, 52)
day_of_month_sin, day_of_month_cos = datetime2circle(df['Dates'].dt.day, df['Dates'].dt.days_in_month)
day_of_year_sin, day_of_year_cos = datetime2circle(df['Dates'].dt.dayofyear, 365)
month_sin, month_cos = datetime2circle(df['Dates'].dt.month, 12)
quarter_sin, quarter_cos = datetime2circle(df['Dates'].dt.quarter, 4)

year = pd.get_dummies(df['Dates'].dt.year, drop_first=True)

police_district = pd.get_dummies(df['PdDistrict'], drop_first=True)

# New feature to indicate if crime occurred at intersection
df['Intersection'] = 1
df.loc[df['Address'].str.contains('Block'), 'Intersection'] = 0

transformed_df = pd.DataFrame()
transformed_df['hour_sin'] = hour_min_sin
transformed_df['hour_cos'] = hour_min_cos
transformed_df['month_sin'] = month_sin
transformed_df['month_cos'] = month_cos
transformed_df['day_of_week_sin'] = day_of_week_sin
transformed_df['day_of_week_cos'] = day_of_week_cos
transformed_df['week_of_year_sin'] = week_of_year_sin
transformed_df['week_of_year_cos'] = week_of_year_cos
transformed_df['day_of_month_sin'] = day_of_month_sin
transformed_df['day_of_month_cos'] = day_of_month_cos
transformed_df['day_of_year_sin'] = day_of_year_sin
transformed_df['day_of_year_cos'] = day_of_year_cos
transformed_df['quarter_sin'] = quarter_sin
transformed_df['quarter_cos'] = quarter_cos
transformed_df['lat'] = df['Y']
transformed_df['long'] = df['X']
transformed_df['intersection'] = df['Intersection']

transformed_df= pd.concat([transformed_df, year, police_district], axis=1)

print(transformed_df.columns)
transformed_df.values.shape
transformed_df.values.dtype

transformed_df['node'] = range(len(transformed_df))
transformed_df.set_index('node', inplace=True)

"""### Normalize lat/long"""

transformed_df['lat'] = (transformed_df['lat'] - transformed_df['lat'].min()) / (transformed_df['lat'].max() - transformed_df['lat'].min()) * 2 - 1
transformed_df['long'] = (transformed_df['long'] - transformed_df['long'].min()) / (transformed_df['long'].max() - transformed_df['long'].min()) * 2 - 1

"""## Cluster lat/long locations with K-Means"""

N_CLUSTERS = 2000

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.cluster import KMeans, MiniBatchKMeans
# 
# kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=0, verbose=True)
# clusters = kmeans.fit_predict(df[['Y','X']])

"""### Assign clusters to nodes"""

df['cluster'] = clusters

"""### Visualize cluster centers"""

pd.DataFrame(kmeans.counts_).describe(percentiles=np.arange(0,1,0.1))

# See http://mattmurray.net/unlocking-the-power-of-geospatial-data-with-geopandas/
geometry = [Point(yx[::-1]) for yx in kmeans.cluster_centers_]
gdf = GeoDataFrame(pd.DataFrame(kmeans.cluster_centers_), geometry=geometry)   
gdf.crs = {'init': 'epsg:4326'}
gdf.plot(figsize=(13,10), marker="o", color="red", markersize=8, edgecolor="black", alpha=0.3)

sf = gpd.read_file('san-francisco.geojson')
sf.crs = {'init': 'epsg:4326'}
sf = sf.rename(columns={'geometry': 'geometry', 'nhood': 'neighborhood_name'}).set_geometry('geometry')
sf.sample(5)

import matplotlib.pyplot as plt
fig, ax = plt.subplots(1, figsize=(13,10))
sf_map = sf.plot(ax=ax, color='gray')
gdf.plot(ax=sf_map, marker="o", color="red", markersize=12, edgecolor="black", alpha=0.6)
ax.set_title("San Francisco crime clusters")

"""## Build feature that counts # of co-occurring spatial neighbor crimes in same cluster"""

from collections import defaultdict

n_neighbors_spatial = {}

groups = df.groupby('Dates')
print(f"# groups: {len(groups)}")

for timeidx, group in tqdm(groups):   
    if len(group.node) > 1:
        # Group nodes/crimes by cluster
        group_clusters_set = set(group.cluster)
        cluster2nodes = defaultdict(list)
        for c,n in zip(group.cluster, group.node):
            cluster2nodes[c].append(n)
            
        # Count # of crimes co-occurring in the same cluster
        for n,c in zip(group.node, group.cluster):
            n_neighbors_spatial[n] = len(cluster2nodes[c]) - 1
    else:
        n = list(group.node)[0]
        n_neighbors_spatial[n] = 0

transformed_df['spatials_count'] = list(map(n_neighbors_spatial.get, transformed_df.index.values))

transformed_df['spatials_count'].describe(percentiles=np.arange(0,1,0.05))

# Standardize spatials count
transformed_df['spatials_count'] = (transformed_df['spatials_count'] - transformed_df['spatials_count'].mean()) / (transformed_df['spatials_count'].std())

"""## Build address features

### Build a vocab of address words and filter stop words
"""

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(stop_words='english', tokenizer=str.split)
vectorizer.fit(df.Address.values)
# print(vectorizer.vocabulary_)

addresses_transformed = vectorizer.transform(df.Address.values)

"""### Record list of address vocab IDs for each sample"""

address_idx = defaultdict(list)
for row, col in zip(*addresses_transformed.nonzero()):
    address_idx[row].append(col)
    
ADDRESS_VOCAB_SIZE = int(max(vectorizer.vocabulary_.values()) + 2)  # Account for padding
ADDRESS_PAD_IDX = ADDRESS_VOCAB_SIZE - 1                            # Last index is pad

"""## Split into Train/Val/Test sets"""

train_mask = torch.BoolTensor((df.split == 'train').values)
test_mask = torch.BoolTensor((df.split == 'test').values)
print(train_mask.sum())
print(test_mask.sum())

from sklearn.preprocessing import LabelEncoder

category_le = LabelEncoder()
_ = category_le.fit(df[df.split=='train']['Category'])
y_labels = torch.full([len(df)], -1).long()
y_labels[train_mask] = torch.tensor(category_le.transform(df['Category'].values[train_mask]))

from sklearn.model_selection import train_test_split

train_idxs = (df.split == 'train').values.nonzero()[0]
train_idxs, val_idxs = train_test_split(train_idxs, test_size=0.1, random_state=0, stratify=y_labels[train_mask])
test_idxs = (df.split == 'test').values.nonzero()[0]

n_train = len(train_idxs)
n_val = len(val_idxs)
print(n_train)
print(n_val)

"""# Build Dataset and Dataloader"""

from more_itertools import flatten

class CrimeDataset(torch.utils.data.Dataset):
    def __init__(self, idxs, clusters, address_idx, Y):
        self.data_idxs = idxs
        self.clusters = clusters
        self.address_idx = address_idx
        self.Y = Y

    def __len__(self):
        return len(self.data_idxs)

    def __getitem__(self, idx):
        node = self.data_idxs[idx]                          
        cluster = self.clusters[node]                       
        address = torch.LongTensor(self.address_idx[node])  
        y = self.Y[node]                                    
        
        return (torch.LongTensor([node]),     # Node ID
                torch.LongTensor([cluster]),  # Cluster ID        
                address,                      # Sequence of address vocab idxs
                y)                            # Target class for node

from collections import namedtuple

Batch = namedtuple('Batch', 'nodes, clusters, addresses, y')

def collate_batch(data_list):
    '''Build batch with node/cluster idxs and padded address idxs'''
    nodes, clusters, addresses, ys = list(zip(*data_list))
    batch_nodes = torch.cat(nodes)
    batch_clusters = torch.cat(clusters)
    batch_addresses_padded = torch.nn.utils.rnn.pad_sequence(
        addresses, batch_first=True, padding_value=ADDRESS_PAD_IDX
    )

    return Batch(batch_nodes,
                 batch_clusters,
                 batch_addresses_padded,
                 torch.LongTensor(ys))

from torch.utils.data import DataLoader

BATCH_SIZE = 128

train_ds = CrimeDataset(train_idxs, clusters, address_idx, y_labels.numpy())
val_ds = CrimeDataset(val_idxs, clusters, address_idx, y_labels.numpy())
test_ds = CrimeDataset(test_idxs, clusters, address_idx, y_labels.numpy())

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,
                            shuffle=True, num_workers=6,
                            collate_fn=lambda data_list: collate_batch(data_list))
val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE * 16,
                            shuffle=False, num_workers=6,
                            collate_fn=lambda data_list: collate_batch(data_list))
test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE * 16,
                            shuffle=False, num_workers=6,
                            collate_fn=lambda data_list: collate_batch(data_list))

"""# Setup model and train"""

class CrimeNet(torch.nn.Module):
    def __init__(self, 
                 X, 
                 hidden_dim,
                 n_classes, 
                 n_clusters=kmeans.n_clusters,
                 cluster_embed_dim=8, 
                 address_vocab_size=ADDRESS_VOCAB_SIZE, 
                 address_pad_idx=ADDRESS_PAD_IDX, 
                 dropout=0.1):
        super(CrimeNet, self).__init__()
        self.address_pad_idx = address_pad_idx
        
        # Create non-trainable embedding table for node features
        X = torch.FloatTensor(X)
        self.node_embed = nn.Embedding.from_pretrained(X, freeze=True)
        
        # Trainable embeddings for cluster IDs
        self.cluster_embed = nn.Embedding(n_clusters, cluster_embed_dim)        
        # Traininable embeddings for address vocab words
        self.address_embed = nn.Embedding(address_vocab_size, hidden_dim, padding_idx=self.address_pad_idx)
        
        # Linear projection layer for nodes to have same hidden dimensionality
        self.node_fc = nn.Linear(X.shape[1], hidden_dim, bias=True)

        # LSTM layer for address sequences
        self.address_rnn = nn.LSTM(
            hidden_dim, hidden_dim, num_layers=1, batch_first=True, dropout=dropout, bidirectional=False
        )
        
        # Fully-connected layer for node + cluster + address vecs
        self.fc = torch.nn.Linear(hidden_dim * 2 + cluster_embed_dim, hidden_dim)
        self.ln = torch.nn.LayerNorm(hidden_dim)
        # Output layer with one prediction per class
        self.out = torch.nn.Linear(hidden_dim, n_classes)
        
        # Regularization shared for all layers
        self.dropout = torch.nn.Dropout(dropout)

    def forward(self, nodes, clusters, addresses, return_probs=False):
        # Look up node features 
        nodes = self.node_embed(nodes)
        # Pass through FC layer
        nodes = F.relu(self.node_fc(nodes))
        
        # Embed the cluster IDs
        clusters = self.cluster_embed(clusters)
        
        # Calculate length of address rows for packing
        lens = (addresses != self.address_pad_idx).sum(1)
        # Embed addresses with vocab IDs
        addresses = self.dropout(self.address_embed(addresses))
        # Pack address sequences to remove padding and improve performance
        addresses_packed = torch.nn.utils.rnn.pack_padded_sequence(
            addresses, lens, batch_first=True, enforce_sorted=False
        )
        # Run the LSTM over address sequences
        output, (hidden, cell) = self.address_rnn(addresses_packed)
        # Use last layer hidden state as the representation for each address
        addresses = hidden[-1]  
        
        # Concatenate node features, address LSTM outs, and cluster embeds
        x = self.dropout(torch.cat([nodes, addresses, clusters], dim=-1))
        # Fully-connected layer with ReLU
        x = self.dropout(self.ln(F.relu(self.fc(x))))
        # Output shape is [batch_size, n_classes]
        x = self.out(x)
        
        if return_probs:
            # Normalize to a probability distribution before returning
            return F.softmax(x, dim=1)
        else:
            # Return logits only
            return x

np.random.seed(0)
torch.manual_seed(0)
model = CrimeNet(transformed_df.values, hidden_dim=256, cluster_embed_dim=8, n_classes=len(category_le.classes_), dropout=0.3)
model

device = 'cuda' if torch.cuda.is_available() else 'cpu'

## Move model to device
model = model.to(device)

# Use Adam optimizer with lr = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
# Reduce LR by 20% after 3 epochs with no improvement
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.8)

best_val_loss = float('inf')
early_stop = 0

for e in range(1, 101):  # 100 epochs max
    t_s = time.perf_counter()
    
    ### TRAINING LOOP ###
    model.train()
    
    train_acc, train_loss = 0, 0   # Running tally of accuracy & loss
    train_preds, train_y = [], []  # Track preds/labels for calculating metrics
    for batch in tqdm(train_loader, total=n_train // BATCH_SIZE):
        optimizer.zero_grad()
        logits = model(  # Forward pass
            batch.nodes.to(device), batch.clusters.to(device), batch.addresses.to(device)
        )
        # Calculate cross entropy loss and perform backward pass
        loss = F.cross_entropy(logits, batch.y.to(device))
        loss.backward()
        optimizer.step()
        
        # Update metrics
        preds = logits.argmax(1)  # Prediction is index of highest logit score
        train_preds.extend(preds.cpu().numpy())
        train_y.extend(batch.y.cpu().numpy())
        train_acc += preds.eq(batch.y.to(device)).sum().item()
        train_loss += loss.item() * batch.y.shape[0]
    # Calculate metrics across whole set
    train_acc  /= n_train    
    train_loss /= n_train
    train_f1_macro = f1_score(train_y, train_preds, average='macro')
    
    ### VALIDATION LOOP ###
    model.eval()
    
    with torch.no_grad():
        val_acc, val_loss = 0, 0
        val_preds, val_y = [], []
        for batch in val_loader:
            logits = model(
                batch.nodes.to(device), batch.clusters.to(device), batch.addresses.to(device)
            )
            loss = F.cross_entropy(logits, batch.y.to(device))
            val_loss += loss.item() * batch.y.shape[0]
            preds = logits.argmax(1)  # Prediction is index of highest logit score
            val_preds.extend(preds.cpu().numpy())
            val_y.extend(batch.y.cpu().numpy())
            val_acc += preds.eq(batch.y.to(device)).sum().item()
        # Calculate validation metrics
        val_acc  /= n_val
        val_loss /= n_val
        val_f1_macro = f1_score(val_y, val_preds, average='macro')
        
        if val_loss < best_val_loss:
            # Save checkpoint if validation loss improves
            best_val_loss = val_loss
            model_path = f"crimenet_epoch_{e}_valloss_{val_loss:.4f}.pt"
            print("\tSaving to", model_path)
            torch.save(model.state_dict(), model_path)
            early_stop = 0
        else:
            # Stop early if validation loss does not improve for 6 epochs
            early_stop += 1
            if early_stop >= 6:
                print(f"No improvement after {early_stop} epochs, stopping early")
                break
    
    scheduler.step(val_loss)
        
    t_e = time.perf_counter()
    print(f"Epoch {e} - "
          f"Train Loss: {train_loss:.4f}  Train Acc: {train_acc:.4f}  Train F1 Macro: {train_f1_macro:.4f}  "
          f"Val Loss: {val_loss:.4f}  Val Acc: {val_acc:.4f}  Val F1 Macro: {val_f1_macro:.4f}  "
          f"Time: {t_e - t_s:.1f}")

"""# Continue training on Val data"""

print(model_path)
model.load_state_dict(torch.load(model_path))

val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE,
                            shuffle=True, num_workers=6,
                            collate_fn=lambda data_list: collate_batch(data_list))

val_epochs = 10

for e in range(0, val_epochs):
    print(e)
    model.train()
    
    for batch in val_loader:
        optimizer.zero_grad()
        logits = model(  # Forward pass
            batch.nodes.to(device), batch.clusters.to(device), batch.addresses.to(device)
        )
        # Calculate cross entropy loss and perform backward pass
        loss = F.cross_entropy(logits, batch.y.to(device))
        loss.backward()
        optimizer.step()

"""# Build submission CSV

## Predict class for each test point
"""

def predict_test(model):
    preds = []
    model.eval()
    with torch.no_grad():
        for batch in test_loader:
            probs = model(
                batch.nodes.to(device), batch.clusters.to(device), batch.addresses.to(device), return_probs=True
            ).cpu().numpy()
            preds.extend(list(probs))
    preds = np.stack(preds)
    return preds

preds_afterval = predict_test(model)

df_submission = pd.DataFrame(columns=category_le.classes_, data=preds_afterval)

df_submission['Id'] = df[df.split=='test']['Id'].values.astype('int')

df_submission.sort_values('Id', inplace=True)

df_submission = df_submission[[ 'Id','ARSON', 'ASSAULT', 'BAD CHECKS', 'BRIBERY', 'BURGLARY',
       'DISORDERLY CONDUCT', 'DRIVING UNDER THE INFLUENCE', 'DRUG/NARCOTIC',
       'DRUNKENNESS', 'EMBEZZLEMENT', 'EXTORTION', 'FAMILY OFFENSES',
       'FORGERY/COUNTERFEITING', 'FRAUD', 'GAMBLING', 'KIDNAPPING',
       'LARCENY/THEFT', 'LIQUOR LAWS', 'LOITERING', 'MISSING PERSON',
       'NON-CRIMINAL', 'OTHER OFFENSES', 'PORNOGRAPHY/OBSCENE MAT',
       'PROSTITUTION', 'RECOVERED VEHICLE', 'ROBBERY', 'RUNAWAY',
       'SECONDARY CODES', 'SEX OFFENSES FORCIBLE', 'SEX OFFENSES NON FORCIBLE',
       'STOLEN PROPERTY', 'SUICIDE', 'SUSPICIOUS OCC', 'TREA', 'TRESPASS',
       'VANDALISM', 'VEHICLE THEFT', 'WARRANTS', 'WEAPON LAWS']]

df_submission.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# df_submission.to_csv(f"submission_ValEpochs_{val_epochs}_ValLoss_{best_val_loss:.4f}.csv.gz", index=False, compression='gzip')